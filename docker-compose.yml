# docker-compose.yml
version: "3.8"

services:
    # Ollama LLM Engine
    ollama:
        image: ollama/ollama:latest
        container_name: research-ollama
        volumes:
            - ollama_models:/root/.ollama
            - ./models:/models
        environment:
            - OLLAMA_MODELS=/models
            - OLLAMA_HOST=0.0.0.0
        ports:
            - "11434:11434"
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: 1
                          capabilities: [gpu]
        networks:
            - research_net
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:11434/"]
            interval: 30s
            timeout: 10s
            retries: 3

    # Redis Cache
    redis:
        image: redis:7-alpine
        container_name: research-redis
        command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lru
        volumes:
            - redis_data:/data
        ports:
            - "6379:6379"
        networks:
            - research_net
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 10s
            timeout: 5s
            retries: 3

    # MCP Server
    mcp-server:
        build:
            context: ./mcp-server
            dockerfile: Dockerfile
        container_name: research-mcp-server
        environment:
            - NODE_ENV=development
            - PORT=3000
            - REDIS_HOST=redis
            - REDIS_PORT=6379
            - OLLAMA_HOST=ollama
            - OLLAMA_PORT=11434
            - OLLAMA_MODEL=mixtral:8x7b-instruct-v0.1-q4_K_M
            - CLIENT_URL=http://localhost:3001
            - CODE_DEV_HOST=code-developer
            - CODE_DEV_PORT=8080
            - DOC_GEN_HOST=doc-generator
            - DOC_GEN_PORT=5001
            - CRAWLER_HOST=research-crawler
            - CRAWLER_PORT=5000
        volumes:
            - ./workspace:/workspace
            - ./logs:/logs
        ports:
            - "3000:3000"
        networks:
            - research_net
        depends_on:
            - redis
            - ollama
        restart: unless-stopped

    # Research Crawler
    research-crawler:
        build:
            context: ./research-crawler
            dockerfile: Dockerfile
        container_name: research-crawler
        environment:
            - REDIS_HOST=redis
            - REDIS_PORT=6379
            - SCHOLAR_API_KEY=${SCHOLAR_API_KEY}
            - IEEE_API_KEY=${IEEE_API_KEY}
            - PORT=5000
        volumes:
            - ./research_data:/data
            - crawler_cache:/cache
        ports:
            - "5000:5000"
        networks:
            - research_net
        depends_on:
            - redis
        restart: unless-stopped

    # Code Developer
    code-developer:
        build:
            context: ./code-developer
            dockerfile: Dockerfile
        container_name: code-developer
        environment:
            - REDIS_HOST=redis
            - REDIS_PORT=6379
            - OLLAMA_HOST=ollama
            - OLLAMA_PORT=11434
            - GIT_USER_NAME=${GIT_USER_NAME:-Research Bot}
            - GIT_USER_EMAIL=${GIT_USER_EMAIL:-bot@research.local}
            - WORKSPACE_PATH=/workspace
            - PORT=8080
        volumes:
            - ./workspace:/workspace
            - ./git_repos:/repos
            - /var/run/docker.sock:/var/run/docker.sock
        ports:
            - "8080:8080"
        networks:
            - research_net
        depends_on:
            - redis
            - ollama
        restart: unless-stopped

    # Document Generator
    doc-generator:
        build:
            context: ./doc-generator
            dockerfile: Dockerfile
        container_name: doc-generator
        environment:
            - REDIS_HOST=redis
            - REDIS_PORT=6379
            - OLLAMA_HOST=ollama
            - OLLAMA_PORT=11434
            - PORT=5001
        volumes:
            - ./workspace:/workspace
            - ./documents:/output
            - ./templates:/templates
        ports:
            - "5001:5001"
        networks:
            - research_net
        depends_on:
            - redis
            - ollama
        restart: unless-stopped

    # Nginx Reverse Proxy (Optional for production)
    nginx:
        image: nginx:alpine
        container_name: research-nginx
        volumes:
            - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
            - ./nginx/ssl:/etc/nginx/ssl:ro
        ports:
            - "80:80"
            - "443:443"
        networks:
            - research_net
        depends_on:
            - mcp-server
        profiles:
            - production

networks:
    research_net:
        driver: bridge

volumes:
    ollama_models:
        driver: local
    redis_data:
        driver: local
    crawler_cache:
        driver: local
